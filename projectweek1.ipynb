{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1599a9ab-0177-4495-b08b-b5d8c7102a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Accuracy: 1.00\n",
      "Epoch 2, Training Accuracy: 1.00\n",
      "Epoch 3, Training Accuracy: 1.00\n",
      "Epoch 4, Training Accuracy: 1.00\n",
      "Epoch 5, Training Accuracy: 1.00\n",
      "Epoch 6, Training Accuracy: 1.00\n",
      "Epoch 7, Training Accuracy: 1.00\n",
      "Epoch 8, Training Accuracy: 1.00\n",
      "Epoch 9, Training Accuracy: 1.00\n",
      "Epoch 10, Training Accuracy: 1.00\n",
      "Epoch 11, Training Accuracy: 1.00\n",
      "Epoch 12, Training Accuracy: 1.00\n",
      "Epoch 13, Training Accuracy: 1.00\n",
      "Epoch 14, Training Accuracy: 1.00\n",
      "Epoch 15, Training Accuracy: 1.00\n",
      "Epoch 16, Training Accuracy: 1.00\n",
      "Epoch 17, Training Accuracy: 1.00\n",
      "Epoch 18, Training Accuracy: 1.00\n",
      "Epoch 19, Training Accuracy: 1.00\n",
      "Epoch 20, Training Accuracy: 1.00\n",
      "Epoch 21, Training Accuracy: 1.00\n",
      "Epoch 22, Training Accuracy: 1.00\n",
      "Epoch 23, Training Accuracy: 1.00\n",
      "Epoch 24, Training Accuracy: 1.00\n",
      "Epoch 25, Training Accuracy: 1.00\n",
      "Epoch 26, Training Accuracy: 1.00\n",
      "Epoch 27, Training Accuracy: 1.00\n",
      "Epoch 28, Training Accuracy: 1.00\n",
      "Epoch 29, Training Accuracy: 1.00\n",
      "Epoch 30, Training Accuracy: 1.00\n",
      "Epoch 31, Training Accuracy: 1.00\n",
      "Epoch 32, Training Accuracy: 1.00\n",
      "Epoch 33, Training Accuracy: 1.00\n",
      "Epoch 34, Training Accuracy: 1.00\n",
      "Epoch 35, Training Accuracy: 1.00\n",
      "Epoch 36, Training Accuracy: 1.00\n",
      "Epoch 37, Training Accuracy: 1.00\n",
      "Epoch 38, Training Accuracy: 1.00\n",
      "Epoch 39, Training Accuracy: 1.00\n",
      "Epoch 40, Training Accuracy: 1.00\n",
      "Epoch 41, Training Accuracy: 1.00\n",
      "Epoch 42, Training Accuracy: 1.00\n",
      "Epoch 43, Training Accuracy: 1.00\n",
      "Epoch 44, Training Accuracy: 1.00\n",
      "Epoch 45, Training Accuracy: 1.00\n",
      "Epoch 46, Training Accuracy: 1.00\n",
      "Epoch 47, Training Accuracy: 1.00\n",
      "Epoch 48, Training Accuracy: 1.00\n",
      "Epoch 49, Training Accuracy: 1.00\n",
      "Epoch 50, Training Accuracy: 1.00\n",
      "Epoch 51, Training Accuracy: 1.00\n",
      "Epoch 52, Training Accuracy: 1.00\n",
      "Epoch 53, Training Accuracy: 1.00\n",
      "Epoch 54, Training Accuracy: 1.00\n",
      "Epoch 55, Training Accuracy: 1.00\n",
      "Epoch 56, Training Accuracy: 1.00\n",
      "Epoch 57, Training Accuracy: 1.00\n",
      "Epoch 58, Training Accuracy: 1.00\n",
      "Epoch 59, Training Accuracy: 1.00\n",
      "Epoch 60, Training Accuracy: 1.00\n",
      "Epoch 61, Training Accuracy: 1.00\n",
      "Epoch 62, Training Accuracy: 1.00\n",
      "Epoch 63, Training Accuracy: 1.00\n",
      "Epoch 64, Training Accuracy: 1.00\n",
      "Epoch 65, Training Accuracy: 1.00\n",
      "Epoch 66, Training Accuracy: 1.00\n",
      "Epoch 67, Training Accuracy: 1.00\n",
      "Epoch 68, Training Accuracy: 1.00\n",
      "Epoch 69, Training Accuracy: 1.00\n",
      "Epoch 70, Training Accuracy: 1.00\n",
      "Epoch 71, Training Accuracy: 1.00\n",
      "Epoch 72, Training Accuracy: 1.00\n",
      "Epoch 73, Training Accuracy: 1.00\n",
      "Epoch 74, Training Accuracy: 1.00\n",
      "Epoch 75, Training Accuracy: 1.00\n",
      "Epoch 76, Training Accuracy: 1.00\n",
      "Epoch 77, Training Accuracy: 1.00\n",
      "Epoch 78, Training Accuracy: 1.00\n",
      "Epoch 79, Training Accuracy: 1.00\n",
      "Epoch 80, Training Accuracy: 1.00\n",
      "Epoch 81, Training Accuracy: 1.00\n",
      "Epoch 82, Training Accuracy: 1.00\n",
      "Epoch 83, Training Accuracy: 1.00\n",
      "Epoch 84, Training Accuracy: 1.00\n",
      "Epoch 85, Training Accuracy: 1.00\n",
      "Epoch 86, Training Accuracy: 1.00\n",
      "Epoch 87, Training Accuracy: 1.00\n",
      "Epoch 88, Training Accuracy: 1.00\n",
      "Epoch 89, Training Accuracy: 1.00\n",
      "Epoch 90, Training Accuracy: 1.00\n",
      "Epoch 91, Training Accuracy: 1.00\n",
      "Epoch 92, Training Accuracy: 1.00\n",
      "Epoch 93, Training Accuracy: 1.00\n",
      "Epoch 94, Training Accuracy: 1.00\n",
      "Epoch 95, Training Accuracy: 1.00\n",
      "Epoch 96, Training Accuracy: 1.00\n",
      "Epoch 97, Training Accuracy: 1.00\n",
      "Epoch 98, Training Accuracy: 1.00\n",
      "Epoch 99, Training Accuracy: 1.00\n",
      "Epoch 100, Training Accuracy: 1.00\n",
      "Validation Accuracy: 1.00\n",
      "Testing Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "1. #1.Implement a perceptron from scratch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Training\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for filename in os.listdir('/home/akhila-bejagam/Pictures/training/'):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        img = Image.open(os.path.join('/home/akhila-bejagam/Pictures/training/', filename))\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "        img_array = np.array(img)\n",
    "        training_data.append(img_array.flatten())\n",
    "        training_labels.append(1)  # Example label, adjust based on your dataset\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.zeros(len(training_data[0]))\n",
    "#print(weights)\n",
    "bias = 0.1\n",
    "\n",
    "# Train the model\n",
    "training_accuracy = []\n",
    "for epoch in range(100):  # adjust the number of epochs as needed\n",
    "    correct = 0\n",
    "    for i in range(len(training_data)):\n",
    "        prediction = np.dot(weights, training_data[i]) + bias\n",
    "        prediction = 1 if prediction >= 0 else 0\n",
    "        error = training_labels[i] - prediction\n",
    "        weights += 0.01 * error * training_data[i]\n",
    "        bias += 0.01 * error\n",
    "        correct += 1 if prediction == training_labels[i] else 0\n",
    "    training_accuracy.append(correct / len(training_data))\n",
    "    print(f\"Epoch {epoch+1}, Training Accuracy: {training_accuracy[-1]:.2f}\")\n",
    "\n",
    "# Validation\n",
    "validation_data = []\n",
    "validation_labels = []\n",
    "for filename in os.listdir('/home/akhila-bejagam/Pictures/validation/'):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        img = Image.open(os.path.join('/home/akhila-bejagam/Pictures/validation/', filename))\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "        img_array = np.array(img)\n",
    "        validation_data.append(img_array.flatten())\n",
    "        validation_labels.append(1)  # Example label, adjust based on your dataset\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "predictions = []\n",
    "for i in range(len(validation_data)):\n",
    "    prediction = np.dot(weights, validation_data[i]) + bias\n",
    "    prediction = 1 if prediction >= 0 else 0\n",
    "    predictions.append(prediction)\n",
    "accuracy = np.mean(predictions == validation_labels)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Testing\n",
    "testing_data = []\n",
    "testing_labels = []\n",
    "for filename in os.listdir('/home/akhila-bejagam/Pictures/testing/'):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        img = Image.open(os.path.join('/home/akhila-bejagam/Pictures/testing/', filename))\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "        img_array = np.array(img)\n",
    "        testing_data.append(img_array.flatten())\n",
    "        testing_labels.append(1)  # Example label, adjust based on your dataset\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "predictions = []\n",
    "for i in range(len(testing_data)):\n",
    "    prediction = np.dot(weights, testing_data[i]) + bias\n",
    "    prediction = 1 if prediction >= 0 else 0\n",
    "    predictions.append(prediction)\n",
    "accuracy = np.mean(predictions == testing_labels)\n",
    "print(f\"Testing Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d940f1-cc03-484e-82dd-569d47cebb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Accuracy: 0.9581, Train Loss: 0.0468\n",
      "Epoch [2/10],Train Accuracy: 1.0000, Train Loss: 0.0001\n",
      "Epoch [3/10],Train Accuracy: 1.0000, Train Loss: 0.0001\n",
      "Epoch [4/10],Train Accuracy: 1.0000, Train Loss: 0.0000\n",
      "Epoch [5/10],Train Accuracy: 1.0000, Train Loss: 0.0000\n",
      "Epoch [6/10],Train Accuracy: 1.0000, Train Loss: 0.0001\n",
      "Epoch [7/10],Train Accuracy: 1.0000, Train Loss: 0.0000\n",
      "Epoch [8/10],Train Accuracy: 1.0000, Train Loss: 0.0000\n",
      "Epoch [9/10],Train Accuracy: 1.0000, Train Loss: 0.0000\n",
      "Epoch [10/10],Train Accuracy: 1.0000, Train Loss: 0.0000\n",
      "Validation Accuracy: 1.0000, Validation Loss: 0.0000\n",
      "Test Accuracy: 1.0000, Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "##2.Build and train a simple neural network using a framework like TensorFlow or PyTorch\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define directories\n",
    "train_dir = '/home/akhila-bejagam/Pictures/training/'\n",
    "val_dir = '/home/akhila-bejagam/Pictures/validation/'\n",
    "test_dir = '/home/akhila-bejagam/Pictures/testing/'\n",
    "\n",
    "# Load images\n",
    "train_images = [os.path.join(train_dir, img) for img in os.listdir(train_dir)]\n",
    "val_images = [os.path.join(val_dir, img) for img in os.listdir(val_dir)]\n",
    "test_images = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader([transform(Image.open(img)) for img in train_images], batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader([transform(Image.open(img)) for img in val_images], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader([transform(Image.open(img)) for img in test_images], batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2)\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.view(-1, 784))\n",
    "        labels = torch.tensor([0] * outputs.size(0))  # Replace with actual labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/10],Train Accuracy: {train_accuracy:.4f}, Train Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    if epoch == 9:  # Print validation results only after the last epoch\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs in val_loader:\n",
    "                val_outputs = model(val_inputs.view(-1, 784))\n",
    "                val_labels = torch.tensor([0] * val_outputs.size(0))  # Replace with actual labels\n",
    "                _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                val_running_loss += val_loss.item()\n",
    "        val_accuracy = val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_running_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_running_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs.view(-1, 784))\n",
    "        labels = torch.tensor([0] * outputs.size(0))  # Replace with actual labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_loss = criterion(outputs, labels)\n",
    "        test_running_loss += test_loss.item()\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_running_loss/len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e14430-3a22-4cd2-81cf-6fc05b0a1b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/training/MNIST/raw/train-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/training/MNIST/raw/train-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/training/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/training/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/training/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/validation/MNIST/raw/train-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/validation/MNIST/raw/train-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/validation/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/validation/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/validation/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/testing/MNIST/raw/train-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/testing/MNIST/raw/train-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/testing/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akhila-bejagam/Pictures/testing/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/akhila-bejagam/Pictures/testing/MNIST/raw\n",
      "\n",
      "Training Accuracy: 99.48 %\n",
      "Test Accuracy: 97.84 %\n",
      "Validation Accuracy: 97.84 %\n"
     ]
    }
   ],
   "source": [
    " #3.Create a multi-layer perceptron (MLP) for digit classification (MNIST dataset)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST('/home/akhila-bejagam/Pictures/training', download=True, train=True, transform=transform)\n",
    "validation_data = datasets.MNIST('/home/akhila-bejagam/Pictures/validation', download=True, train=False, transform=transform)\n",
    "test_data = datasets.MNIST('/home/akhila-bejagam/Pictures/testing', download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # input layer (28x28) -> hidden layer (128)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),  # hidden layer (128) -> hidden layer (64)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)  # hidden layer (64) -> output layer (10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    #print(f'Epoch {epoch+1}, Loss: {running_loss/i}')\n",
    "\n",
    "# Evaluate on training set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ac8727-1b18-44ca-a6a1-cadd10938784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with L1 regularization...\n",
      "Epoch 1, Loss: 2.2766783287127814, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.6385390063126882, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.6442995568116506, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.6445342799027761, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.6429651180903116, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.6408714056015015, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.6386340806881586, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.6363537559906641, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.6340682158867518, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.6317739884058634, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with L1 regularization: 100.0%\n",
      "Training with L2 regularization...\n",
      "Epoch 1, Loss: 1.2135847806930542, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.6299373457829157, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.6323277950286865, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.6314852784077326, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.6296017120281855, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.6274151553710302, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.6251499851544698, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.6228642165660858, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.6205736845731735, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.6182880252599716, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with L2 regularization: 100.0%\n",
      "Training with Dropout regularization...\n",
      "Epoch 1, Loss: 0.7857209238979065, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 3.9637291108599965e-07, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 2.793964763251703e-09, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 1.5522041985072121e-10, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Dropout regularization: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#4Experiment with different regularization techniques on a neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Define paths to datasets\n",
    "training_path = \"/home/akhila-bejagam/Pictures/training\"\n",
    "validation_path = \"/home/akhila-bejagam/Pictures/validation\"\n",
    "testing_path = \"/home/akhila-bejagam/Pictures/testing\"\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Paths to datasets\n",
    "datasets = [\n",
    "    {'name': 'train', 'path': training_path},\n",
    "    {'name': 'validation', 'path': validation_path},\n",
    "    {'name': 'test', 'path': testing_path}\n",
    "]\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "data_loaders = {}\n",
    "for dataset in datasets:\n",
    "    images = [f for f in os.listdir(dataset['path']) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    data = []\n",
    "    labels = []\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(dataset['path'], img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(torch.tensor(0))  # Assuming all labels are 0 for simplicity\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    data_tensor = torch.stack(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    data_loaders[dataset['name']] = DataLoader(TensorDataset(data_tensor, labels_tensor), batch_size=64, shuffle=dataset['name'] == 'train')\n",
    "\n",
    "# Neural network model\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(224 * 224 * 3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Regularization techniques\n",
    "regularization_techniques = ['L1', 'L2', 'Dropout']\n",
    "\n",
    "for technique in regularization_techniques:\n",
    "    print(f\"Training with {technique} regularization...\")\n",
    "    \n",
    "    # Reset model parameters and optimizer\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Dropout):\n",
    "            module.p = 0.5\n",
    "            \n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # Train the network\n",
    "    num_epochs = 10 # Increase number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        net.train()  # Set the model to train mode\n",
    "        for dataset_name in ['train']:\n",
    "            for i, (inputs, labels) in enumerate(data_loaders[dataset_name], 0):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Apply regularization for L1 and L2\n",
    "                if technique == 'L1' or technique == 'L2':\n",
    "                    l1_lambda = 0.00001  # Adjust regularization strength\n",
    "                    l2_lambda = 0.0001   # Adjust regularization strength\n",
    "                    l1_reg = torch.tensor(0., requires_grad=False)\n",
    "                    l2_reg = torch.tensor(0., requires_grad=False)\n",
    "                    for param in net.parameters():\n",
    "                        l1_reg += torch.norm(param, 1)\n",
    "                        l2_reg += torch.norm(param, 2)\n",
    "                    loss += l1_lambda * l1_reg + l2_lambda * l2_reg\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loaders['train']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in data_loaders['validation']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            validation_accuracy = 100 * correct / total\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss / len(data_loaders[\"train\"])}, Train Accuracy: {train_accuracy}%, Validation Accuracy: {validation_accuracy}%')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loaders['test']:\n",
    "            inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy with {technique} regularization: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712440c8-d42b-4c3b-8aac-fb4d666df634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, SGD Training Accuracy: 1.00\n",
      "Epoch 2, SGD Training Accuracy: 1.00\n",
      "Epoch 3, SGD Training Accuracy: 1.00\n",
      "Epoch 4, SGD Training Accuracy: 1.00\n",
      "Epoch 5, SGD Training Accuracy: 1.00\n",
      "SGD Validation Accuracy: 1.00\n",
      "SGD Testing Accuracy: 1.00\n",
      "Epoch 1, MBGD Training Accuracy: 1.00\n",
      "Epoch 2, MBGD Training Accuracy: 1.00\n",
      "Epoch 3, MBGD Training Accuracy: 1.00\n",
      "Epoch 4, MBGD Training Accuracy: 1.00\n",
      "Epoch 5, MBGD Training Accuracy: 1.00\n",
      "MBGD Validation Accuracy: 1.00\n",
      "MBGD Testing Accuracy: 1.00\n",
      "Epoch 1, GD Training Accuracy: 1.00\n",
      "Epoch 2, GD Training Accuracy: 1.00\n",
      "Epoch 3, GD Training Accuracy: 1.00\n",
      "Epoch 4, GD Training Accuracy: 1.00\n",
      "Epoch 5, GD Training Accuracy: 1.00\n",
      "GD Validation Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#5.Compare performance with various optimization algorithms\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths to your data folders\n",
    "training_dir = '/home/akhila-bejagam/Pictures/training'\n",
    "validation_dir = '/home/akhila-bejagam/Pictures/validation'\n",
    "testing_dir = '/home/akhila-bejagam/Pictures/testing'\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Load and preprocess training, validation, and testing data\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for filename in os.listdir(training_dir):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(training_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "        img_array = np.array(img)\n",
    "        training_data.append(img_array.flatten())\n",
    "        training_labels.append(1)  # Example label, adjust based on your dataset\n",
    "\n",
    "validation_data = []\n",
    "validation_labels = []\n",
    "for filename in os.listdir(validation_dir):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(validation_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "        img_array = np.array(img)\n",
    "        validation_data.append(img_array.flatten())\n",
    "        validation_labels.append(1)  # Example label, adjust based on your dataset\n",
    "\n",
    "testing_data = []\n",
    "testing_labels = []\n",
    "for filename in os.listdir(testing_dir):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(testing_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "        img_array = np.array(img)\n",
    "        testing_data.append(img_array.flatten())\n",
    "        testing_labels.append(1)  # Example label, adjust based on your dataset\n",
    "\n",
    "# SGD Training\n",
    "weights_sgd = np.random.randn(len(training_data[0]))\n",
    "bias_sgd = 0.1\n",
    "training_accuracy_sgd = []\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    for i in range(len(training_data)):\n",
    "        prediction = np.dot(weights_sgd, training_data[i]) + bias_sgd\n",
    "        prediction = 1 if prediction >= 0 else 0\n",
    "        error = training_labels[i] - prediction\n",
    "        weights_sgd += learning_rate * error * training_data[i]\n",
    "        bias_sgd += learning_rate * error\n",
    "        correct += 1 if prediction == training_labels[i] else 0\n",
    "    training_accuracy_sgd.append(correct / len(training_data))\n",
    "    print(f\"Epoch {epoch+1}, SGD Training Accuracy: {training_accuracy_sgd[-1]:.2f}\")\n",
    "\n",
    "# Evaluate on validation data with SGD\n",
    "predicted_labels = (np.dot(np.expand_dims(weights_sgd, axis=0), np.transpose(validation_data)) + bias_sgd) >= 0\n",
    "predicted_labels = predicted_labels.astype(int)\n",
    "validation_accuracy_sgd = np.mean(predicted_labels == validation_labels)\n",
    "\n",
    "print(f\"SGD Validation Accuracy: {validation_accuracy_sgd:.2f}\")\n",
    "\n",
    "# Evaluate on testing data with SGD\n",
    "predicted_labels = (np.dot(np.expand_dims(weights_sgd, axis=0), np.transpose(testing_data)) + bias_sgd) >= 0\n",
    "testing_accuracy_sgd = np.mean(np.all(predicted_labels == testing_labels, axis=1))\n",
    "\n",
    "\n",
    "print(f\"SGD Testing Accuracy: {testing_accuracy_sgd:.2f}\")\n",
    "\n",
    "# Mini-Batch Gradient Descent (MBGD) Training\n",
    "weights_mbgd = np.random.randn(len(training_data[0]))\n",
    "bias_mbgd = 0.1\n",
    "training_accuracy_mbgd = []\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    for i in range(0, len(training_data), 32):  # Batch size of 32\n",
    "        batch_data = training_data[i:i+32]\n",
    "        batch_labels = training_labels[i:i+32]\n",
    "        for j in range(len(batch_data)):\n",
    "            prediction = np.dot(weights_mbgd, batch_data[j]) + bias_mbgd\n",
    "            prediction = 1 if prediction >= 0 else 0\n",
    "            error = batch_labels[j] - prediction\n",
    "            weights_mbgd += learning_rate * error * batch_data[j]\n",
    "            bias_mbgd += learning_rate * error\n",
    "            correct += 1 if prediction == batch_labels[j] else 0\n",
    "    training_accuracy_mbgd.append(correct / len(training_data))\n",
    "    print(f\"Epoch {epoch+1}, MBGD Training Accuracy: {training_accuracy_mbgd[-1]:.2f}\")\n",
    "\n",
    "# Evaluate on validation data with MBGD\n",
    "predicted_labels = (np.dot(np.expand_dims(weights_mbgd, axis=0), np.transpose(validation_data)) + bias_mbgd) >= 0\n",
    "validation_accuracy_mbgd = np.mean(np.all(predicted_labels == validation_labels, axis=1))\n",
    "print(f\"MBGD Validation Accuracy: {validation_accuracy_mbgd:.2f}\")\n",
    "\n",
    "# Evaluate on testing data with MBGD\n",
    "\n",
    "predicted_labels = (np.dot(np.expand_dims(weights_mbgd, axis=0), np.transpose(testing_data)) + bias_mbgd) >= 0\n",
    "testing_accuracy_mbgd = np.mean(np.all(predicted_labels == testing_labels, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MBGD Testing Accuracy: {testing_accuracy_mbgd:.2f}\")\n",
    "\n",
    "# Gradient Descent (GD) Training\n",
    "weights_gd = np.random.randn(len(training_data[0]))\n",
    "bias_gd = 0.1\n",
    "training_accuracy_gd = []\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    for i in range(len(training_data)):\n",
    "        prediction = np.dot(weights_gd, training_data[i]) + bias_gd\n",
    "        prediction = 1 if prediction >= 0 else 0\n",
    "        error = training_labels[i] - prediction\n",
    "        weights_gd += learning_rate * error * training_data[i]\n",
    "        bias_gd += learning_rate * error\n",
    "        correct += 1 if prediction == training_labels[i] else 0\n",
    "    training_accuracy_gd.append(correct / len(training_data))\n",
    "    print(f\"Epoch {epoch+1}, GD Training Accuracy: {training_accuracy_gd[-1]:.2f}\")\n",
    "\n",
    "#Evaluate on validation data with GD\n",
    "predicted_labels = (np.dot(np.expand_dims(weights_gd, axis=0), np.transpose(validation_data)) + bias_gd) >= 0\n",
    "validation_accuracy_GD = np.mean(np.all(predicted_labels == validation_labels, axis=1))\n",
    "print(f\"GD Validation Accuracy: {validation_accuracy_mbgd:.2f}\")\n",
    "\n",
    "# Evaluate on testing data with GD\n",
    "\n",
    "predicted_labels = (np.dot(np.expand_dims(weights_gd, axis=0), np.transpose(testing_data)) + bias_gd) >= 0\n",
    "testing_accuracy_GD = np.mean(np.all(predicted_labels == testing_labels, axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0abd9-c554-4d7d-91f9-fae542ac5017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
